<div align="center">

![poster](https://github.com/YangXuanyi/Multi-Agent-GPT/assets/83216339/b111f8f3-3b14-42dc-89b3-9a26c7a7deeb)

##

A multimodal expert assistant GPT platform built using RAG+agent. It integrates tools for modalities such as text, images, and audio. Support local deployment and private database construction.

![Code](https://img.shields.io/badge/Code-python-purple)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) [![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat#pic_center)](http://makeapullrequest.com)
![license](https://img.shields.io/badge/License-MIT-{å³åŠéƒ¨åˆ†é¢œè‰²})


[Web Video](https://github.com/YangXuanyi/Multi-Agent-GPT/assets/83216339/30e76e97-d02e-4a18-b5e1-896be99d5564)

</div>

## ğŸ’¡ 1 RoadMap

`1` Basic Function
   - [x] Single/multi turn chat
   - [x] Multimodal information display and interaction
   - [x] Agent
   - [x] Tools
      - [x] Web searching
      - [x] Image generation
      - [x] Image caption
      - [ ] audio-to-text
      - [ ] text-to-audio
      - [ ] Video caption
   - [ ] RAG
      - [ ] Private database
      - [ ] Offline deployment

   

`2` Supporting Information Modality
   - [x] text
   - [x] image
   - [ ] audio
   - [ ] video

`3` Model Interface API
   - [x] ChatGPT
   - [x] Dalle
   - [x] Google-Search
   - [x] BLIP

## ğŸ‘¨â€ğŸ’» 2 Development

Project technology stack: Python + torch + langchain + gradio

### **âš¡ 2.1 Installation**

1. Create a virtual environment in Anaconda:

  ```
  conda create -n agent python=3.10
  ```

2. Enter the virtual environment and Install related dependency packages:
  
  ```
  conda activate agent
  ```

  ```
  pip install -r ./requirements.txt
  ```

3. Install the BLIP model locally, open the [BLIP website](https://huggingface.co/Salesforce/blip-image-captioning-large), and download all files to ``Models/BLIP``.


4. Follow the prompts to configure the key for the API that needs to be used in the `.env`.



### **ğŸ’» 2.2 Demo**

Multi Agent GPT provides UI interface interaction, allowing users to launch agents and achieve intelligent conversations by running the ``web.py``:

```
python ./web.py
```
The program will run a local URL: http://XXX. Open using a local browser to see the UI interface:

<div align="center">

![demo](https://github.com/YangXuanyi/Multi-Agent-GPT/assets/83216339/82444566-c7db-41ab-b471-cc3fba5ada82)

</div>

### **ğŸ“» 2.3 News**

#### 1 Chat_with_Image

By integrating the BLIP model, agents can understand image information and provide high-quality dialogue information.



## ğŸ—„ï¸ 3 Structure

```
- .env
- Agents/
  - openai_agents.py  #ç”¨æ¥å®šä¹‰åŸºäºgpt3.5çš„agent
- Database/
- Docs/
- Imgs/
  - Show/                #å­˜å‚¨ä¸€äº›ç¤ºä¾‹å›¾ç‰‡
- Models
  - BLIP                 #å›¾åƒç†è§£å¤§æ¨¡å‹
- Tools/
  - ImageCaption.py      #åŸºäºBLIPçš„å›¾åƒç†è§£å·¥å…·
  - ImageGeneration.py  #å®šä¹‰äº†ä¸€ä¸ªåŸºäºopenai dalleçš„æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„å·¥å…·
  - search.py            #åŸºäºGoogle-searchçš„è”ç½‘æœç´¢å·¥å…·
- Utils/
  - data_io.py
  - stdio.py            #å®ç°äº†å¦‚ä½•æˆªè·å½“å‰ç¨‹åºçš„æ—¥å¿—ä¿¡æ¯ï¼Œä¸»è¦æ˜¯ç”¨æ¥è·å–agentçš„verboseä¿¡æ¯
  - utils_image.py      #å…³äºå›¾åƒå¤„ç†çš„ä¸€äº›åŠŸèƒ½å‡½æ•°
  - utils_json.py       #ä»å·²æœ‰çš„logæ—¥å¿—ä¿¡æ¯ä¸­æå–ç›¸å…³çš„æœ‰ç”¨å­—æ®µ(æœåŠ¡stdio) 
- python_new_funciton.py #å¼€å‘è¿‡ç¨‹ä¸­çš„æµ‹è¯•æ–‡ä»¶
- readme.md
- requirements.txt
- web.py                 #ä¸»è¿è¡Œæ–‡ä»¶

```